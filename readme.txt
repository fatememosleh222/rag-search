Architecture:
frontend (React or simple HTML)
        |
        v
[ Flask API ]
  â”œâ”€â”€ /ingest        # ETL: pull open data, clean, chunk, store
  â”œâ”€â”€ /index         # compute embeddings, upsert into vector DB
  â”œâ”€â”€ /search        # retrieve (hybrid lexical + vector)
  â”œâ”€â”€ /rag           # retrieve + generate grounded answer (LLM)
  â””â”€â”€ /health
        |
        v
[Storage]
  â”œâ”€â”€ data/raw/clean        # CSV/JSON/GeoJSON, cache API responses
  â”œâ”€â”€ Postgres/SQLite       # metadata & relational facts
  â””â”€â”€ Qdrant (vector DB)    # embeddings + payloads

[Retrieval]
  â”œâ”€â”€ BM25 (whoosh/elastic-lite)  # optional lexical index
  â”œâ”€â”€ Qdrant vector search        # cosine similarity
  â””â”€â”€ Reranker (optional)         # cross-encoder for relevance

[LLM & Embeddings]
  â”œâ”€â”€ sentence-transformers       # e.g., all-MiniLM-L6-v2 (free)
  â””â”€â”€ Ollama local LLM            # e.g., mistral / llama3

[DAG/Automation]
  â””â”€â”€ simple cron or Prefect (free) for scheduled ingestion






How to Run:


$env:OPENAQ_API_KEY = "test"
python -m app.pipelines.ingest_openaq

python -m app.pipelines.ingest_openaq
python -m app.pipelines.ingest_osm
python -m app.pipelines.ingest_gtfs


installation and build

python scripts/build_index.py


check build 

# List the output files
Get-Item .\data\index\faiss.index, .\data\index\meta.jsonl | Format-Table Name,Length,LastWriteTime

# Count how many metadata rows were written (lines = vectors)
(Get-Content .\data\index\meta.jsonl).Count



run

$env:OPENAQ_API_KEY = "aaf4de9cc76e9fb71694308b5a4da761f1929d543a23b0432f869b782e8df688"
python -m app.pipelines.ingest_openaq


results = store.search(query_embedding, top_k=8)


run

1: run flask 

python -c "from app.app import app; app.run(host='0.0.0.0', port=8000)"


2. ask query : 
curl -X POST http://localhost:8000/search/ `  -H "Content-Type: application/json" `-d '{"query":"Where are bike lanes near downtown?"}'


3.python request: 
import requests
r = requests.post("http://localhost:8000/rag/", json={"query":"Where are bike lanes near downtown and what is PM2.5 there now?"})
print(r.status_code, r.json())



Second run way: 
 .\scripts\run_server.ps1   


How it works in your code:


Retrieve:

Your question â†’ converted to an embedding (vector).
FAISS searches your indexed chunks (from OpenAQ, OSM, GTFS).
Top relevant chunks are returned.



Augment:

These chunks are combined into a context string.
The context + your question â†’ sent to the LLM.



Generate:

The LLM (via Ollama) produces an answer based only on that context.
It includes citations from the chunks.







# RAG-Search ğŸ”ğŸ§ 

An advanced Retrieval-Augmented Generation (RAG) system designed to enhance large language model responses by grounding them in relevant, external knowledge sources.

## ğŸ¯ Problem Statement
Large Language Models can generate fluent answers but may:
- Hallucinate facts
- Lack access to up-to-date or domain-specific knowledge
- Fail on knowledge-intensive queries

This project addresses these limitations by integrating a retrieval layer that injects relevant context into the generation pipeline.

## ğŸ§  System Architecture
The system follows a modular RAG pipeline:

Query  
â†’ Embedding Generation  
â†’ Vector Similarity Search  
â†’ Context Selection  
â†’ LLM Prompt Augmentation  
â†’ Answer Generation  

This design separates retrieval from generation, enabling scalability and component-level optimization.

## ğŸ” Retrieval Strategy
- Dense vector embeddings for semantic search
- Similarity-based ranking (top-k retrieval)
- Context window optimization to reduce prompt noise

The retrieval layer is intentionally decoupled to allow:
- Swapping vector databases
- Experimenting with embedding models
- Domain-specific tuning

## ğŸ§ª Generation Strategy
- Prompt construction includes:
  - Retrieved context
  - Query intent
  - Guardrails against hallucination
- Generation focuses on **grounded responses**, not creative completion

## ğŸ›  Tech Stack
- Python
- LLM APIs
- Embedding models
- Vector search (FAISS / equivalent)
- Modular prompt engineering

## âš™ï¸ Design Decisions
- **Why RAG instead of fine-tuning?**  
  Faster iteration, lower cost, easier knowledge updates.
- **Why modular components?**  
  Enables independent optimization and testing.
- **Why similarity search?**  
  Robust semantic retrieval over keyword matching.

## ğŸ“ˆ Use Cases
- Enterprise knowledge assistants
- Technical documentation Q&A
- Research paper retrieval
- Internal search systems

## âš ï¸ Limitations
- Retrieval quality depends on embedding choice
- Context length constrained by LLM limits
- Performance varies by domain density

## ğŸš€ Future Work
- Hybrid retrieval (keyword + dense)
- Reranking with cross-encoders
- Query rewriting and intent detection
- Evaluation metrics for retrieval quality

## ğŸ‘©â€ğŸ’» Author
Fatemeh Mosleh  
MSc Artificial Intelligence (Distinction)



